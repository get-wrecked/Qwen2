{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2afc33-17ad-4782-a567-eea5fb7875a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d31c3c-a45b-4875-9584-b531dcfe049b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers peft huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ba0eb1-8287-4141-abf0-f5c2dcfb5344",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install peft deepspeed optimum accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d06b99c-cee8-4fc1-8f0a-e38adb1a3872",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8059f17-130a-40e7-b079-386c12587193",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA L4\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print(\"No GPU available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "116273ef-6937-4bcb-abef-269d1d85346f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/karthick/finetuning/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /home/karthick/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from peft import PeftModel, PeftConfig\n",
    "from huggingface_hub import login\n",
    "\n",
    "login(token=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dbc77bff-716a-4cfe-ae4f-ccabd671cf8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model_name = \"Qwen/Qwen2-1.5B\"\n",
    "adapter_model_name = \"tryhighlight/qwen2-1.5B-ocr-task-detection\"\n",
    "\n",
    "config = PeftConfig.from_pretrained(adapter_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "14676617-bd18-4755-bf7a-3672a50a3338",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_name,\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f1adabec-b1ba-4934-8dbb-69ad56bc8520",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PeftModel.from_pretrained(model, adapter_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "52821abc-e465-475b-b323-e020376374e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b46ac838-2f08-4cf8-a8b3-981b68fadb23",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.special_tokens_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "152a6b8e-864f-4720-8638-670b87c26ecc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): Qwen2ForCausalLM(\n",
       "      (model): Qwen2Model(\n",
       "        (embed_tokens): Embedding(151936, 1536)\n",
       "        (layers): ModuleList(\n",
       "          (0-27): 28 x Qwen2DecoderLayer(\n",
       "            (self_attn): Qwen2SdpaAttention(\n",
       "              (q_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=1536, out_features=1536, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1536, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=1536, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=1536, out_features=256, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1536, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=256, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=1536, out_features=256, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1536, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=256, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=1536, out_features=1536, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1536, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=1536, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (rotary_emb): Qwen2RotaryEmbedding()\n",
       "            )\n",
       "            (mlp): Qwen2MLP(\n",
       "              (gate_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=1536, out_features=8960, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1536, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=8960, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (up_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=1536, out_features=8960, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1536, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=8960, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=8960, out_features=1536, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=8960, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=1536, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): Qwen2RMSNorm()\n",
       "            (post_attention_layernorm): Qwen2RMSNorm()\n",
       "          )\n",
       "        )\n",
       "        (norm): Qwen2RMSNorm()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=1536, out_features=151936, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()  # Set the model to evaluation mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6c975330-7cfd-4044-afae-bb83ac484a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_chat_template(system_prompt, user_name, ocr_text, max_len):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": \"My name is \" + user_name + \" \\n\" + ocr_text},\n",
    "    ]\n",
    "    tokenized_output = tokenizer.apply_chat_template(\n",
    "                            messages,\n",
    "                            tokenize=True,\n",
    "                            add_generation_prompt=False,\n",
    "                            padding=False,\n",
    "                            max_length=max_len,\n",
    "                            truncation=True,\n",
    "                    )\n",
    "    input_ids = torch.tensor(tokenized_output, dtype=torch.int)\n",
    "    print (input_ids.shape)\n",
    "    return input_ids\n",
    "\n",
    "def generate_response(system_prompt, user_name, ocr_text):\n",
    "    max_len = 4096\n",
    "    input_ids = apply_chat_template(system_prompt, user_name, ocr_text, max_len)\n",
    "    input_ids = input_ids.to(model.device)\n",
    "    # Ensure input_ids is 2D\n",
    "    if input_ids.dim() == 1:\n",
    "        input_ids = input_ids.unsqueeze(0)\n",
    "    # Create an attention mask for the input_ids\n",
    "    attention_mask = torch.ones_like(input_ids, device=model.device)\n",
    "\n",
    "    # Check dimensions\n",
    "    print(\"Input IDs Shape:\", input_ids.shape)\n",
    "    print(\"Attention Mask Shape:\", attention_mask.shape)\n",
    "\n",
    "    outputs = model.generate(input_ids, attention_mask=attention_mask, max_new_tokens=50, pad_token_id=tokenizer.pad_token_id)\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "02f6f0f0-9390-4af4-8ef9-3dd5473dafc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "system_prompt = \"You are a helpful AI assistant. User will provide full name followed by the OCR content of his/her computer screen. Looking at the OCR, first detect if there are any email or messaging or any other kind of conversations in it. If yes, then detect if there are any TODOs that the above user has to complete as a result of the conversation. If yes, just provide a short single line task that can be directly added to the todo list. If there is no conversation detected or no task detected in the conversation as a TODO, just output the exact phrase 'No task'.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bc7abeec-a9ae-4d5f-8e92-56c10bd6978f",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_name = \"@Karthick\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "07730aa4-0e7a-4427-86d6-5c0bd2e84677",
   "metadata": {},
   "outputs": [],
   "source": [
    "ocr_text = \"8 hhightignt (Chane -Medal-s' x |) | | | I | | I |\\nΓé¼ > G@ 8% appslack.com/client/T037W748A/CO6ADHR1G3F?cdn_fallback=1 * O10:\\n& ┬⌐ Search Medal Q @\\nMedal v = Gi @highlight ┬⌐ _ bttos:/app.amplitude.com/analytics/medal/chart/new/fmitanwb BBB x 63┬╗ Weamas\\n@Atharva what's the latest on the non serverless endpoints so the warm up time is better - and what's up with handling lang context? The APIs shoul \\= nati ~┬Ñ this nd users should be informed.\\ne ┬⌐ Threads @Josh / @Sam when will screenshots work normally in HL Chat? Do we need to do more designs to unblock team next week? What work do we nee. Mond┬Ñ-July 15th ~ cht chat to a great UX quickly?\\nE> Drafts & sent @Maximilian Maksutovic when can we integrate conversations with floaty? What's blocking it from being able to open from attachments? How are we going to fit it into onboarding? These questions all need answers and progress towards\\nHome @Mark what's holding us back from more marketing channels? (edited)\\na ~ External Connections BD BB 2 eeviies Last realy 3 days ago |\\nMs ##-_ medal-akamai @ Pim 74sane\\n) _* shated-medat-honeycomb @lulian i had a note from last week that you'd ship the auth skipping - did that happen last week?\\npetty ΓÇö-┬Ñ Channels BBD ┬╗ vies Last ents das aso\\nOW sees a Pim 10:21am\\nQ dt beckend replied to a thread: IMG_1200\\nLater @lulian *\\n# builds 2 @\\n1 Γé¼\\n4 chatgpt\\nsts 4 community g Sukkrit 6:37 eM\\noo <M Hiteam, down with fever, will rest up for the day\\n- weed New\\nvon fF eacombesuer - Canna:\\n# data-and-experiments ┬« Brandon ΓÇ£> 6:26 pm\\n3D Tried using Highlight Chat on my windows this weekend and it just responded to me with an empty message. I'l see if I can reproduce it\\n4 desktop-frontend\\na me\\n# desktop-github\\nper BB 2 enea\\n& highlight a Pim 7.03PM\\n# incidents replied to a thread: \\\"Cmd .\\\" And now highlight is frozen\\nA * @Karthick can we get to the bottom of this?\\n┬⌐ incoming-medal-\\nΓÇö e\\n4 Issue-tracking .\\new newer replies\\n& leaders\\n& Pim 7:10PM\\na ED replied to a thread: \\\"Cmd.:\\\" And now highlight is frozen\\n# move we already are not any new users - the least we can do is make sure our existing users aren't churning due to crahses, gotta prioritize this\\n# nyclife\\n# random\\n+ essage @highlight\\n# releases ┬░\\nfA #  sentry-alerts tha O@ Ge OB\\nLA it ser ┬Ñ More unreads\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5a2ff84f-6d7a-4c84-bd0e-c5dd103bfe2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([867])\n",
      "Input IDs Shape: torch.Size([1, 867])\n",
      "Attention Mask Shape: torch.Size([1, 867])\n",
      "Generated response: system\n",
      "You are a helpful ai assistant. User will provide full name followed by the OCR content of his/her computer screen. Looking at the OCR, first detect if there are any email or messaging or any other kind of conversations in it. If yes, then detect if there are any TODOs that the above user has to complete as a result of the conversation. If yes, just provide a short single line task that can be directly added to the todo list. If there is no converstaion detected or no task detected in the conversation as a TODO, just output the exact phrase \"No task\".\n",
      "user\n",
      "My name is @Karthick \n",
      "8 hhightignt (Chane -Medal-s' x |) | | | I | | I |\n",
      "Γé¼ > G@ 8% appslack.com/client/T037W748A/CO6ADHR1G3F?cdn_fallback=1 * O10:\n",
      "& ┬⌐ Search Medal Q @\n",
      "Medal v = Gi @highlight ┬⌐ _ bttos:/app.amplitude.com/analytics/medal/chart/new/fmitanwb BBB x 63┬╗ Weamas\n",
      "@Atharva what's the latest on the non serverless endpoints so the warm up time is better - and what's up with handling lang context? The APIs shoul \\= nati ~┬Ñ this nd users should be informed.\n",
      "e ┬⌐ Threads @Josh / @Sam when will screenshots work normally in HL Chat? Do we need to do more designs to unblock team next week? What work do we nee. Mond┬Ñ-July 15th ~ cht chat to a great UX quickly?\n",
      "E> Drafts & sent @Maximilian Maksutovic when can we integrate conversations with floaty? What's blocking it from being able to open from attachments? How are we going to fit it into onboarding? These questions all need answers and progress towards\n",
      "Home @Mark what's holding us back from more marketing channels? (edited)\n",
      "a ~ External Connections BD BB 2 eeviies Last realy 3 days ago |\n",
      "Ms ##-_ medal-akamai @ Pim 74sane\n",
      ") _* shated-medat-honeycomb @lulian i had a note from last week that you'd ship the auth skipping - did that happen last week?\n",
      "petty ΓÇö-┬Ñ Channels BBD ┬╗ vies Last ents das aso\n",
      "OW sees a Pim 10:21am\n",
      "Q dt beckend replied to a thread: IMG_1200\n",
      "Later @lulian *\n",
      "# builds 2 @\n",
      "1 Γé¼\n",
      "4 chatgpt\n",
      "sts 4 community g Sukkrit 6:37 eM\n",
      "oo <M Hiteam, down with fever, will rest up for the day\n",
      "- weed New\n",
      "von fF eacombesuer - Canna:\n",
      "# data-and-experiments ┬« Brandon ΓÇ£> 6:26 pm\n",
      "3D Tried using Highlight Chat on my windows this weekend and it just responded to me with an empty message. I'l see if I can reproduce it\n",
      "4 desktop-frontend\n",
      "a me\n",
      "# desktop-github\n",
      "per BB 2 enea\n",
      "& highlight a Pim 7.03PM\n",
      "# incidents replied to a thread: \"Cmd .\" And now highlight is frozen\n",
      "A * @Karthick can we get to the bottom of this?\n",
      "┬⌐ incoming-medal-\n",
      "ΓÇö e\n",
      "4 Issue-tracking .\n",
      "ew newer replies\n",
      "& leaders\n",
      "& Pim 7:10PM\n",
      "a ED replied to a thread: \"Cmd.:\" And now highlight is frozen\n",
      "# move we already are not any new users - the least we can do is make sure our existing users aren't churning due to crahses, gotta prioritize this\n",
      "# nyclife\n",
      "# random\n",
      "+ essage @highlight\n",
      "# releases ┬░\n",
      "fA #  sentry-alerts tha O@ Ge OB\n",
      "LA it ser ┬Ñ More unreads\n",
      "\n",
      "assistant\n",
      "No task\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = generate_response(system_prompt, user_name, ocr_text)\n",
    "print(\"Generated response:\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "82da61e2-f932-4f4b-a1eb-16e3cb1eb011",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['NAME', 'OCR', 'TASK'],\n",
      "    num_rows: 77568\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "from datasets import load_dataset\n",
    "from typing import Dict, List, Optional\n",
    "\n",
    "max_len = 4096\n",
    "\n",
    "dataset_name = \"tryhighlight/ocr_task_dataset\"\n",
    "#Importing the dataset\n",
    "input_dataset = load_dataset(dataset_name, split=\"all\")\n",
    "print (input_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d91548-7dd2-4de1-b8d1-4338524f6fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_task = 0\n",
    "for row in input_dataset:\n",
    "    if row[\"TASK\"] == \"No task\":\n",
    "        no_task += 1\n",
    "print (no_task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "be9190c7-a5c3-4b2c-abed-58747e304480",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No task\n",
      "No task\n",
      "No task\n",
      "Task : Check voice mail for a message from Ed regarding the storage.\n",
      "No task\n",
      "No task\n",
      "No task\n",
      "No task\n",
      "No task\n",
      "No task\n",
      "No task\n",
      "No task\n",
      "No task\n",
      "No task\n",
      "No task\n",
      "No task\n",
      "No task\n",
      "No task\n",
      "No task\n",
      "No task\n",
      "Task : Set up Monarch Steel Company in global system.\n",
      "No task\n",
      "Task : Provide important dates to MAWhitt@aol.com\n",
      "Task : Process and approve or reject outstanding business expenses in iPayIt and XMS by 2:00 p.m. Monday, December 3, 2001.\n",
      "Task : No task detected.\n",
      "Task : Approve access request for gulay.soykok@enron.com.\n",
      "No task\n",
      "No task\n",
      "Task : Complete practice questions for Crack Spread, BTU, and Spark Spread.\n",
      "Task : Resend the email version which Salt River sent to you\n",
      "No task\n",
      "No task\n",
      "No task\n",
      "Task : Approve access request for erik.simpson@enron.com for Market Data Kobra Power.\n",
      "No task\n",
      "No task\n",
      "No task\n",
      "No task\n",
      "Task : Complete self assessment in PAS by Friday, May 17, 2002.\n",
      "No task\n",
      "Task : Determine the demand charges for path #17 and #21.\n",
      "No task\n",
      "No task\n",
      "Task : Forward the notice related to the coal marketing agency agreement to Alan Aronowitz.\n",
      "Task : Make a pledge to the United Way campaign via the HR global system.\n",
      "Task : Send a list of procedures run by UBS people that should be run by the estate team to Stephanie Hopkins.\n",
      "No task\n",
      "No task\n",
      "Task : Schedule another trial for Henwood's online applications.\n",
      "No task\n",
      "Task : Explore with ENA whether additional information about Calpine's trading strategy and limitations on Miller's duties alleviates ENA's concerns.\n",
      "No task\n",
      "No task\n",
      "Task : Send Julie my phone number and mailing address.\n",
      "No task\n",
      "No task\n",
      "Task : Send demand letters to the counterparties listed by Shari Mao.\n",
      "No task\n",
      "No task\n",
      "No task\n",
      "Task : Please let me know if there is another letter or number missing\n",
      "No task\n",
      "No task\n",
      "No task\n",
      "No task\n",
      "No task\n",
      "No task\n",
      "No task\n",
      "Task : Attend OATI meeting on Nov 1st from 9:00 AM to 2:00 PM in EB3127\n",
      "No task\n",
      "No task\n",
      "No task\n",
      "No task\n",
      "No task\n",
      "No task\n",
      "No task\n",
      "No task\n",
      "No task\n",
      "No task\n",
      "No task\n",
      "No task\n",
      "Task : Let me know if you need further information.\n",
      "No task\n",
      "Task : Identify areas in your business units that have been vacated and call the Help Desk at x36300, option 2.\n",
      "No task\n",
      "Task : Provide updated contact information to enron.houston.ias@us.pwcglobal.com\n",
      "No task\n",
      "No task\n",
      "No task\n",
      "No task\n",
      "No task\n",
      "No task\n",
      "No task\n",
      "No task\n",
      "No task\n",
      "No task\n",
      "No task\n",
      "No task\n",
      "Task : Send at least one representative from the regional desk to the 11:30 call in Mt. Bachelor to discuss the list.\n",
      "No task\n"
     ]
    }
   ],
   "source": [
    "test = input_dataset.shuffle(seed=65).select(range(100))\n",
    "for row in test:\n",
    "    task_prefix = \"Task : \" if row[\"TASK\"] != \"No task\" else \"\"\n",
    "    task = task_prefix + row[\"TASK\"]\n",
    "    print (task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "45352990-cf01-4354-aeb6-bd58822142da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the dataset: 77568 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|███████████████████████████████████████████████████████████| 77568/77568 [03:18<00:00, 390.28 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the dataset after filtering: 77543 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Get the size of the dataset\n",
    "dataset_size = len(input_dataset)\n",
    "print(f\"Size of the dataset: {dataset_size} samples\")\n",
    "system_prompt = \"You are a helpful ai assistant. User will provide full name followed by the OCR content of his/her computer screen. Looking at the OCR, first detect if there are any email or messaging or any other kind of conversations in it. If yes, then detect if there are any TODOs that the above user has to complete as a result of the conversation. If yes, just provide a short single line task that can be directly added to the todo list. If there is no converstaion detected or no task detected in the conversation as a TODO, just output the exact phrase \\\"No task\\\".\"\n",
    "#    dataset = dataset.shuffle(seed=65).select(range(10000)) # Only use 1000 samples for quick demo\n",
    "def check_length(row):\n",
    "    # This function checks if the tokenized length is within the max length allowed\n",
    "    input_content = tokenizer.encode(system_prompt + row[\"NAME\"] + row[\"OCR\"] + row[\"TASK\"],\n",
    "                                        add_special_tokens=True,\n",
    "                                        truncation=False,\n",
    "                                        return_length=True,\n",
    "                                        max_length=None)\n",
    "    return len(input_content) <= max_len - 20 # extra 20 tokens for the chat template\n",
    "\n",
    "# Filter the dataset to exclude entries that are too long\n",
    "filtered_dataset = input_dataset.filter(check_length)\n",
    "print(f\"Size of the dataset after filtering: {len(filtered_dataset)} samples\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5d07ebd8-8b07-49aa-aa11-0efbb09dde2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column names in the dataset: ['NAME', 'OCR', 'TASK', 'input_ids']\n"
     ]
    }
   ],
   "source": [
    "def format_chat_template(row):\n",
    "    row_json = [\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": \"My name is \" + row[\"NAME\"] + \" \\n\" + row[\"OCR\"]}]\n",
    "    tokenized_output = tokenizer.apply_chat_template(\n",
    "                            row_json,\n",
    "                            tokenize=True,\n",
    "                            add_generation_prompt=False,\n",
    "                            padding=False,\n",
    "                            max_length=max_len,\n",
    "                            truncation=True,\n",
    "                    )\n",
    "    input_ids = torch.tensor(tokenized_output, dtype=torch.int)\n",
    "\n",
    "    return dict(\n",
    "                input_ids=input_ids\n",
    "            )\n",
    "\n",
    "dataset = filtered_dataset.map(\n",
    "    format_chat_template,\n",
    "    num_proc=8,\n",
    ")\n",
    "print(\"Column names in the dataset:\", dataset.column_names)\n",
    "\n",
    "dataset = dataset.train_test_split(test_size=0.1, seed = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2f8f7dd6-7525-40b3-8402-2b3ae811b274",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['NAME', 'OCR', 'TASK', 'input_ids'],\n",
       "    num_rows: 69788\n",
       "})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "094d0177-abfd-4c36-a15a-9c5a2f7895f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dataset=dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "769cd440-d98c-4714-a494-4547537c0afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dataset = eval_dataset.shuffle(seed=65).select(range(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2d1dff49-5d77-44a8-a7f8-4ad16ee6f0e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['NAME', 'OCR', 'TASK', 'input_ids'],\n",
       "    num_rows: 100\n",
       "})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341c0d01-54b7-4412-94d7-4e5da6b900e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dataset[0]['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b0d0e1c2-5507-4c1e-9eb4-6ee92381a540",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected output :  No task\n",
      "Inference output :  No task\n",
      "\n",
      "Expected output :  No task\n",
      "Inference output :  No task\n",
      "\n",
      "Expected output :  No task\n",
      "Inference output :  No task\n",
      "\n",
      "Expected output :  No task\n",
      "Inference output :  No task\n",
      "\n",
      "Expected output :  No task\n",
      "Inference output :  No task\n",
      "\n",
      "Expected output :  Attend the video conference on January 3, 2002.\n",
      "Inference output :  No task\n",
      "\n",
      "Expected output :  No task\n",
      "Inference output :  No task\n",
      "\n",
      "Expected output :  No task\n",
      "Inference output :  No task\n",
      "\n",
      "Expected output :  No task\n",
      "Inference output :  No task\n",
      "\n",
      "Expected output :  No task\n",
      "Inference output :  No task\n",
      "\n",
      "Expected output :  No task\n",
      "Inference output :  No task\n",
      "\n",
      "Expected output :  No task\n",
      "Inference output :  No task\n",
      "\n",
      "Expected output :  Confirm the FUGG imbalances and clarify their status.\n",
      "Inference output :  Task : Confirm the correct imbalances on the G/L for FUGG and let Rita know what the true imbalances should be.\n",
      "\n",
      "Expected output :  No task\n",
      "Inference output :  Task : Please let Stacey Richardson know if you have any questions about the new Committed Reserves/First Purchaser GTC.\n",
      "\n",
      "Expected output :  No task\n",
      "Inference output :  No task\n",
      "\n",
      "Expected output :  No task\n",
      "Inference output :  Task : Forward the drafted amendment to M.E. Booker at Kerr-McGee with a copy to Max.\n",
      "\n",
      "Expected output :  No task\n",
      "Inference output :  No task\n",
      "\n",
      "Expected output :  No task\n",
      "Inference output :  No task\n",
      "\n",
      "Expected output :  No task\n",
      "Inference output :  No task\n",
      "\n",
      "Expected output :  Mark the 23rd of October for Compaq's 3rd quarter earnings conference call\n",
      "Inference output :  No task\n",
      "\n",
      "Expected output :  No task\n",
      "Inference output :  No task\n",
      "\n",
      "Expected output :  No task\n",
      "Inference output :  No task\n",
      "\n",
      "Expected output :  Review Zimin's write-up on using option pricing for EOL price setting.\n",
      "Inference output :  Task : Write up the program SnagIt for printing window.\n",
      "\n",
      "Expected output :  No task\n",
      "Inference output :  No task\n",
      "\n",
      "Expected output :  No task\n",
      "Inference output :  No task\n",
      "\n",
      "Expected output :  Schedule a meeting with Michelle Yee.\n",
      "Inference output :  Task : Let Michelle Yee know when you have a free moment.\n",
      "\n",
      "Expected output :  No task\n",
      "Inference output :  No task\n",
      "\n",
      "Expected output :  No task\n",
      "Inference output :  No task\n",
      "\n",
      "Expected output :  No task\n",
      "Inference output :  No task\n",
      "\n",
      "Expected output :  No task\n",
      "Inference output :  No task\n",
      "\n",
      "Expected output :  No task\n",
      "Inference output :  Task : Sign on to your secure banking session and submit your request.\n",
      "\n",
      "Expected output :  No task\n",
      "Inference output :  No task\n",
      "\n",
      "Expected output :  No task\n",
      "Inference output :  No task\n",
      "\n",
      "Expected output :  No task\n",
      "Inference output :  No task\n",
      "\n",
      "Expected output :  Ensure all confidential information is marked on every page.\n",
      "Inference output :  Task : Advise on the use of the confidential stamp in the proposed confidentiality agreement.\n",
      "\n",
      "Expected output :  Identify who in Calgary should work the Petro document.\n",
      "Inference output :  Task : Let me know if you have any questions.\n",
      "\n",
      "Expected output :  No task\n",
      "Inference output :  No task\n",
      "\n",
      "Expected output :  No task\n",
      "Inference output :  No task\n",
      "\n",
      "Expected output :  Confirm the details requested by Cecilia Cheung regarding the ENA-WT-SOCAL book and active financial deals transfer.\n",
      "Inference output :  Task : Confirm the following details for the deals move: CP ID, move active Financial Deals, commodity code, cutoff date, and use ENA CP ID 1305 as the other side counterparty.\n",
      "\n",
      "Expected output :  No task\n",
      "Inference output :  No task\n",
      "\n",
      "Expected output :  No task\n",
      "Inference output :  No task\n",
      "\n",
      "Expected output :  No task\n",
      "Inference output :  No task\n",
      "\n",
      "Expected output :  Schedule a meeting with Troy Denetsosie after the 1st of the month.\n",
      "Inference output :  Task : Schedule a meeting with Troy Denetsosie to discuss Transport 101.\n",
      "\n",
      "Expected output :  No task\n",
      "Inference output :  No task\n",
      "\n",
      "Expected output :  No task\n",
      "Inference output :  No task\n",
      "\n",
      "Expected output :  No task\n",
      "Inference output :  No task\n",
      "\n",
      "Expected output :  No task\n",
      "Inference output :  No task\n",
      "\n",
      "Expected output :  Return the original Bank of America letter of credit number 3024519 to Fortis Capital Corp.\n",
      "Inference output :  Task : Return the original Bank of America letter of credit number 3024519 to Fortis Capital Corp.\n",
      "\n",
      "Expected output :  No task\n",
      "Inference output :  No task\n",
      "\n",
      "Expected output :  No task\n",
      "Inference output :  No task\n",
      "\n",
      "Expected output :  No task\n",
      "Inference output :  No task\n",
      "\n",
      "Expected output :  No task\n",
      "Inference output :  No task\n",
      "\n",
      "Expected output :  Attend classes from October 15th to October 26th in room 3321 from 3 to 4.\n",
      "Inference output :  No task\n",
      "\n",
      "Expected output :  Plan to attend the STP meeting tomorrow at 10:30 am with current and future projects, timelines, and resources.\n",
      "Inference output :  No task\n",
      "\n",
      "Expected output :  No task\n",
      "Inference output :  No task\n",
      "\n",
      "Expected output :  No task\n",
      "Inference output :  No task\n",
      "\n",
      "Expected output :  No task\n",
      "Inference output :  No task\n",
      "\n",
      "Expected output :  No task\n",
      "Inference output :  No task\n",
      "\n",
      "Expected output :  No task\n",
      "Inference output :  No task\n",
      "\n",
      "Expected output :  No task\n",
      "Inference output :  No task\n",
      "\n",
      "Expected output :  No task\n",
      "Inference output :  No task\n",
      "\n",
      "Expected output :  Print and complete the form sent by Stephen M. Benotti.\n",
      "Inference output :  No task\n",
      "\n",
      "Expected output :  No task\n",
      "Inference output :  No task\n",
      "\n",
      "Expected output :  No task\n",
      "Inference output :  No task\n",
      "\n",
      "Expected output :  Forward a list of issues and concerns prior to the meeting.\n",
      "Inference output :  No task\n",
      "\n",
      "Expected output :  No task\n",
      "Inference output :  No task\n",
      "\n",
      "Expected output :  No task\n",
      "Inference output :  Task : Confirm population of books for Day 1 of testing.\n",
      "\n",
      "Expected output :  No task\n",
      "Inference output :  Task : Follow up with Mark Friedfeld regarding the resume drop deadline for the companies.\n",
      "\n",
      "Expected output :  No task\n",
      "Inference output :  Task : Manual intervention required for HourAhead schedule download failure.\n",
      "\n",
      "Expected output :  No task\n",
      "Inference output :  No task\n",
      "\n",
      "Expected output :  No task\n",
      "Inference output :  No task\n",
      "\n",
      "Expected output :  Submit the weekly assignment by EOD Friday, 10/26/01.\n",
      "Inference output :  No task\n",
      "\n",
      "Expected output :  Edit the Copy Physical Macro in the P&L's saved in the Netco/Regions folder.\n",
      "Inference output :  No task\n",
      "\n",
      "Expected output :  No task\n",
      "Inference output :  No task\n",
      "\n",
      "Expected output :  No task\n",
      "Inference output :  No task\n",
      "\n",
      "Expected output :  No task\n",
      "Inference output :  No task\n",
      "\n",
      "Expected output :  No task\n",
      "Inference output :  No task\n",
      "\n",
      "Expected output :  No task\n",
      "Inference output :  No task\n",
      "\n",
      "Expected output :  No task\n",
      "Inference output :  No task\n",
      "\n",
      "Expected output :  Set up a meeting to provide complete matrices on licensing and utility contracts early during the week beginning Dec 3.\n",
      "Inference output :  Task : Provide complete matrices on licensing and utility contracts to Dan.\n",
      "\n",
      "Expected output :  No task\n",
      "Inference output :  No task\n",
      "\n",
      "Expected output :  No task\n",
      "Inference output :  No task\n",
      "\n",
      "Expected output :  No task\n",
      "Inference output :  No task\n",
      "\n",
      "Expected output :  No task\n",
      "Inference output :  No task\n",
      "\n",
      "Expected output :  No task\n",
      "Inference output :  No task\n",
      "\n",
      "Expected output :  No task\n",
      "Inference output :  No task\n",
      "\n",
      "Expected output :  No task\n",
      "Inference output :  No task\n",
      "\n",
      "Expected output :  Forward all bullet items to Kim via email before Friday morning.\n",
      "Inference output :  No task\n",
      "\n",
      "Expected output :  No task\n",
      "Inference output :  No task\n",
      "\n",
      "Expected output :  No task\n",
      "Inference output :  No task\n",
      "\n",
      "Expected output :  No task\n",
      "Inference output :  No task\n",
      "\n",
      "Expected output :  No task\n",
      "Inference output :  Task : Please let Anne Labbe know if you need additional info.\n",
      "\n",
      "Expected output :  No task\n",
      "Inference output :  No task\n",
      "\n",
      "Expected output :  Ask Paul if the additional detailed term sheets from Tom Blair have been provided yet.\n",
      "Inference output :  Task : Review the term sheet and provide feedback to Paul Puchot.\n",
      "\n",
      "Expected output :  No task\n",
      "Inference output :  No task\n",
      "\n",
      "Expected output :  No task\n",
      "Inference output :  No task\n",
      "\n",
      "Expected output :  No task\n",
      "Inference output :  No task\n",
      "\n",
      "Expected output :  No task\n",
      "Inference output :  No task\n",
      "\n",
      "Expected output :  No task\n",
      "Inference output :  No task\n",
      "\n",
      "Expected output :  No task\n",
      "Inference output :  No task\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for row in eval_dataset:\n",
    "    input_ids = row['input_ids']\n",
    "    input_ids = torch.tensor(input_ids, dtype=torch.int)\n",
    "    input_ids = input_ids.to(model.device)\n",
    "    # Ensure input_ids is 2D\n",
    "    if input_ids.dim() == 1:\n",
    "        input_ids = input_ids.unsqueeze(0)\n",
    "    # Create an attention mask for the input_ids\n",
    "    attention_mask = torch.ones_like(input_ids, device=model.device)\n",
    "\n",
    "    outputs = model.generate(input_ids, attention_mask=attention_mask, max_new_tokens=50, pad_token_id=tokenizer.pad_token_id)\n",
    "    outputs[input_ids.shape[1]:]\n",
    "    text_output = tokenizer.decode(outputs[0][input_ids.shape[1]:], skip_special_tokens=True)\n",
    "    print ('Expected output : ', row['TASK'])\n",
    "    cleaned_output = text_output.replace(\"assistant\\n\", \"\")\n",
    "    print ('Inference output : ', cleaned_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173ae0de-06dd-4845-a8c9-8a625d7e6501",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_text = tokenizer.decode(200)\n",
    "token_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5bd3efd-4892-49cd-912e-3396a5472f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "print (input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2877b3cf-ae35-41f7-a9b5-59caf358ff09",
   "metadata": {},
   "outputs": [],
   "source": [
    "print (attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e082d8f-7fbc-4b0a-9e03-131b228975fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "eos_indices = (input_ids == tokenizer.eos_token_id).nonzero(as_tuple=True)[0]\n",
    "len(eos_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06faf16d-44e4-4d51-94d0-09b21e23e8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "eos_indices[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebdcae17-8f6f-4e96-b678-e11ba0c501c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_eos_index = eos_indices[0]\n",
    "labels[first_eos_index] = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6329a06-f5b2-4c8e-b04e-7a9c83811dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc47746-d0cf-4705-b754-3460d49b4ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_mask[first_eos_index] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "757315fe-e196-41de-ba4d-79e7a667a304",
   "metadata": {},
   "outputs": [],
   "source": [
    "print (attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95103d45-e7ab-4e7c-9b7a-c70036608c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019ef3b4-3fe2-4f64-a7f5-7ba7095e5aaf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

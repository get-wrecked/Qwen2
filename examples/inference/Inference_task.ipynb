{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2afc33-17ad-4782-a567-eea5fb7875a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d31c3c-a45b-4875-9584-b531dcfe049b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers peft huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ba0eb1-8287-4141-abf0-f5c2dcfb5344",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install peft deepspeed optimum accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d06b99c-cee8-4fc1-8f0a-e38adb1a3872",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8059f17-130a-40e7-b079-386c12587193",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print(\"No GPU available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "116273ef-6937-4bcb-abef-269d1d85346f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/karthick/finetuning/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /home/karthick/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from peft import PeftModel, PeftConfig\n",
    "from huggingface_hub import login\n",
    "\n",
    "login(token=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf36c06d-36f5-435c-ac2d-5824e8674c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model_name = \"Qwen/Qwen2-1.5B\"\n",
    "adapter_model_name = \"tryhighlight/qwen2-1.5B-task-detection\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_name,\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "720a6fec-92a8-4818-9189-350c6742767b",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model_name = \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "adapter_model_name = \"tryhighlight/phi3-mini-task-dataset\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_name,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=\"auto\",  \n",
    "    trust_remote_code=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "367dfe0a-dcb2-42b4-bb98-f6b98639d3b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|███████████████████████████████████████████| 4/4 [00:08<00:00,  2.07s/it]\n"
     ]
    }
   ],
   "source": [
    "base_model_name = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "adapter_model_name = \"/home/karthick/Qwen2/examples/sft/llama-3-8b-task-detect/checkpoint-9000\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_name,\n",
    "    device_map=\"auto\",\n",
    "    attn_implementation = \"eager\",\n",
    "    quantization_config=BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dbc77bff-716a-4cfe-ae4f-ccabd671cf8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = PeftConfig.from_pretrained(adapter_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f1adabec-b1ba-4934-8dbb-69ad56bc8520",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for PeftModelForCausalLM:\n\tsize mismatch for base_model.model.model.embed_tokens.weight: copying a param with shape torch.Size([128258, 4096]) from checkpoint, the shape in current model is torch.Size([128256, 4096]).\n\tsize mismatch for base_model.model.lm_head.weight: copying a param with shape torch.Size([128258, 4096]) from checkpoint, the shape in current model is torch.Size([128256, 4096]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mPeftModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_model_name\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/finetuning/lib/python3.9/site-packages/peft/peft_model.py:430\u001b[0m, in \u001b[0;36mPeftModel.from_pretrained\u001b[0;34m(cls, model, model_id, adapter_name, is_trainable, config, **kwargs)\u001b[0m\n\u001b[1;32m    428\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    429\u001b[0m     model \u001b[38;5;241m=\u001b[39m MODEL_TYPE_TO_PEFT_MODEL_MAPPING[config\u001b[38;5;241m.\u001b[39mtask_type](model, config, adapter_name)\n\u001b[0;32m--> 430\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_adapter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_trainable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_trainable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    431\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m~/finetuning/lib/python3.9/site-packages/peft/peft_model.py:988\u001b[0m, in \u001b[0;36mPeftModel.load_adapter\u001b[0;34m(self, model_id, adapter_name, is_trainable, torch_device, **kwargs)\u001b[0m\n\u001b[1;32m    986\u001b[0m \u001b[38;5;66;03m# load the weights into the model\u001b[39;00m\n\u001b[1;32m    987\u001b[0m ignore_mismatched_sizes \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore_mismatched_sizes\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m--> 988\u001b[0m load_result \u001b[38;5;241m=\u001b[39m \u001b[43mset_peft_model_state_dict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    989\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapters_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madapter_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_mismatched_sizes\u001b[49m\n\u001b[1;32m    990\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    991\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    992\u001b[0m     (\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhf_device_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    993\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mset\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhf_device_map\u001b[38;5;241m.\u001b[39mvalues())\u001b[38;5;241m.\u001b[39mintersection({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdisk\u001b[39m\u001b[38;5;124m\"\u001b[39m})) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    994\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpeft_config) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    995\u001b[0m ):\n\u001b[1;32m    996\u001b[0m     device_map \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdevice_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/finetuning/lib/python3.9/site-packages/peft/utils/save_and_load.py:353\u001b[0m, in \u001b[0;36mset_peft_model_state_dict\u001b[0;34m(model, peft_model_state_dict, adapter_name, ignore_mismatched_sizes)\u001b[0m\n\u001b[1;32m    348\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m\n\u001b[1;32m    350\u001b[0m peft_model_state_dict, mismatched_keys \u001b[38;5;241m=\u001b[39m _find_mismatched_keys(\n\u001b[1;32m    351\u001b[0m     model, peft_model_state_dict, ignore_mismatched_sizes\u001b[38;5;241m=\u001b[39mignore_mismatched_sizes\n\u001b[1;32m    352\u001b[0m )\n\u001b[0;32m--> 353\u001b[0m load_result \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpeft_model_state_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config\u001b[38;5;241m.\u001b[39mis_prompt_learning:\n\u001b[1;32m    355\u001b[0m     model\u001b[38;5;241m.\u001b[39mprompt_encoder[adapter_name]\u001b[38;5;241m.\u001b[39membedding\u001b[38;5;241m.\u001b[39mload_state_dict(\n\u001b[1;32m    356\u001b[0m         {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweight\u001b[39m\u001b[38;5;124m\"\u001b[39m: peft_model_state_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt_embeddings\u001b[39m\u001b[38;5;124m\"\u001b[39m]}, strict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    357\u001b[0m     )\n",
      "File \u001b[0;32m~/finetuning/lib/python3.9/site-packages/torch/nn/modules/module.py:2189\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2184\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   2185\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2186\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[1;32m   2188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2189\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2190\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   2191\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for PeftModelForCausalLM:\n\tsize mismatch for base_model.model.model.embed_tokens.weight: copying a param with shape torch.Size([128258, 4096]) from checkpoint, the shape in current model is torch.Size([128256, 4096]).\n\tsize mismatch for base_model.model.lm_head.weight: copying a param with shape torch.Size([128258, 4096]) from checkpoint, the shape in current model is torch.Size([128256, 4096])."
     ]
    }
   ],
   "source": [
    "model = PeftModel.from_pretrained(model, adapter_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52821abc-e465-475b-b323-e020376374e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b46ac838-2f08-4cf8-a8b3-981b68fadb23",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.special_tokens_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152a6b8e-864f-4720-8638-670b87c26ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()  # Set the model to evaluation mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f6f0f0-9390-4af4-8ef9-3dd5473dafc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "system_prompt = \"You are a helpful ai assistant. User will provide full name followed by some email or messaging conversation seen on his/her computer screen. Looking at the conversation, detect if there are any TODOs that the above user has to complete as a result of the conversation. If yes, just provide the short single line task that can be directly added to the todo list. If there is no task detected as a TODO, just output the exact phrase \\\"No task\\\". If the conversation is about a promotional or advertisement related, please output \\\"No task\\\". If the conversation is directed or addressed to someone else, then output \\\"No task\\\".\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07730aa4-0e7a-4427-86d6-5c0bd2e84677",
   "metadata": {},
   "outputs": [],
   "source": [
    "ocr_text = \"8 hhightignt (Chane -Medal-s' x |) | | | I | | I |\\nΓé¼ > G@ 8% appslack.com/client/T037W748A/CO6ADHR1G3F?cdn_fallback=1 * O10:\\n& ┬⌐ Search Medal Q @\\nMedal v = Gi @highlight ┬⌐ _ bttos:/app.amplitude.com/analytics/medal/chart/new/fmitanwb BBB x 63┬╗ Weamas\\n@Atharva what's the latest on the non serverless endpoints so the warm up time is better - and what's up with handling lang context? The APIs shoul \\= nati ~┬Ñ this nd users should be informed.\\ne ┬⌐ Threads @Josh / @Sam when will screenshots work normally in HL Chat? Do we need to do more designs to unblock team next week? What work do we nee. Mond┬Ñ-July 15th ~ cht chat to a great UX quickly?\\nE> Drafts & sent @Maximilian Maksutovic when can we integrate conversations with floaty? What's blocking it from being able to open from attachments? How are we going to fit it into onboarding? These questions all need answers and progress towards\\nHome @Mark what's holding us back from more marketing channels? (edited)\\na ~ External Connections BD BB 2 eeviies Last realy 3 days ago |\\nMs ##-_ medal-akamai @ Pim 74sane\\n) _* shated-medat-honeycomb @lulian i had a note from last week that you'd ship the auth skipping - did that happen last week?\\npetty ΓÇö-┬Ñ Channels BBD ┬╗ vies Last ents das aso\\nOW sees a Pim 10:21am\\nQ dt beckend replied to a thread: IMG_1200\\nLater @lulian *\\n# builds 2 @\\n1 Γé¼\\n4 chatgpt\\nsts 4 community g Sukkrit 6:37 eM\\noo <M Hiteam, down with fever, will rest up for the day\\n- weed New\\nvon fF eacombesuer - Canna:\\n# data-and-experiments ┬« Brandon ΓÇ£> 6:26 pm\\n3D Tried using Highlight Chat on my windows this weekend and it just responded to me with an empty message. I'l see if I can reproduce it\\n4 desktop-frontend\\na me\\n# desktop-github\\nper BB 2 enea\\n& highlight a Pim 7.03PM\\n# incidents replied to a thread: \\\"Cmd .\\\" And now highlight is frozen\\nA * @Karthick can we get to the bottom of this?\\n┬⌐ incoming-medal-\\nΓÇö e\\n4 Issue-tracking .\\new newer replies\\n& leaders\\n& Pim 7:10PM\\na ED replied to a thread: \\\"Cmd.:\\\" And now highlight is frozen\\n# move we already are not any new users - the least we can do is make sure our existing users aren't churning due to crahses, gotta prioritize this\\n# nyclife\\n# random\\n+ essage @highlight\\n# releases ┬░\\nfA #  sentry-alerts tha O@ Ge OB\\nLA it ser ┬Ñ More unreads\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82da61e2-f932-4f4b-a1eb-16e3cb1eb011",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from datasets import load_dataset\n",
    "from typing import Dict, List, Optional\n",
    "\n",
    "max_len = 1024\n",
    "\n",
    "dataset_name = \"tryhighlight/task_dataset\"\n",
    "#Importing the dataset\n",
    "input_dataset = load_dataset(dataset_name, split=\"train\")\n",
    "print (input_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d91548-7dd2-4de1-b8d1-4338524f6fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_task = 0\n",
    "for row in input_dataset:\n",
    "    if row[\"TASK\"] == \"No task\":\n",
    "        no_task += 1\n",
    "print (no_task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be9190c7-a5c3-4b2c-abed-58747e304480",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = input_dataset.shuffle(seed=65).select(range(100))\n",
    "for row in test:\n",
    "    task = row[\"TASK\"]\n",
    "    print (task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45352990-cf01-4354-aeb6-bd58822142da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the size of the dataset\n",
    "dataset_size = len(input_dataset)\n",
    "print(f\"Size of the dataset: {dataset_size} samples\")\n",
    "#    dataset = dataset.shuffle(seed=65).select(range(10000)) # Only use 1000 samples for quick demo\n",
    "def check_length(row):\n",
    "    # This function checks if the tokenized length is within the max length allowed\n",
    "    input_content = tokenizer.encode(system_prompt + row[\"NAME\"] + row[\"CONVERSATION\"] + row[\"TASK\"],\n",
    "                                        add_special_tokens=True,\n",
    "                                        truncation=False,\n",
    "                                        return_length=True,\n",
    "                                        max_length=None)\n",
    "    return len(input_content) <= max_len - 20 # extra 20 tokens for the chat template\n",
    "\n",
    "# Filter the dataset to exclude entries that are too long\n",
    "filtered_dataset = input_dataset.filter(check_length)\n",
    "print(f\"Size of the dataset after filtering: {len(filtered_dataset)} samples\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d07ebd8-8b07-49aa-aa11-0efbb09dde2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_chat_template(row):\n",
    "    row_json = [\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": \"My name is \" + row[\"NAME\"] + \" \\n\" + row[\"CONVERSATION\"]}]\n",
    "    tokenized_output = tokenizer.apply_chat_template(\n",
    "                            row_json,\n",
    "                            tokenize=True,\n",
    "                            add_generation_prompt=False,\n",
    "                            padding=False,\n",
    "                            max_length=max_len,\n",
    "                            truncation=True,\n",
    "                    )\n",
    "    input_ids = torch.tensor(tokenized_output, dtype=torch.int)\n",
    "\n",
    "    return dict(\n",
    "                input_ids=input_ids, user=\"My name is \" + row[\"NAME\"] + \" \\n\" + row[\"CONVERSATION\"]\n",
    "            )\n",
    "\n",
    "dataset = filtered_dataset.map(\n",
    "    format_chat_template,\n",
    "    num_proc=8,\n",
    ")\n",
    "print(\"Column names in the dataset:\", dataset.column_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f8f7dd6-7525-40b3-8402-2b3ae811b274",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "769cd440-d98c-4714-a494-4547537c0afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dataset = dataset.shuffle(seed=65).select(range(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1dff49-5d77-44a8-a7f8-4ad16ee6f0e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d0e1c2-5507-4c1e-9eb4-6ee92381a540",
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in eval_dataset:\n",
    "    input_ids = row['input_ids']\n",
    "    input_ids = torch.tensor(input_ids, dtype=torch.int)\n",
    "    input_ids = input_ids.to(model.device)\n",
    "    # Ensure input_ids is 2D\n",
    "    if input_ids.dim() == 1:\n",
    "        input_ids = input_ids.unsqueeze(0)\n",
    "    # Create an attention mask for the input_ids\n",
    "    attention_mask = torch.ones_like(input_ids, device=model.device)\n",
    "\n",
    "    outputs = model.generate(input_ids, attention_mask=attention_mask, max_new_tokens=50, pad_token_id=tokenizer.pad_token_id, do_sample=True, temperature=1.0)\n",
    "    outputs[input_ids.shape[1]:]\n",
    "    text_output = tokenizer.decode(outputs[0][input_ids.shape[1]:], skip_special_tokens=True)\n",
    "    #print ('input : ', row['user'])\n",
    "    print ('Expected output : ', row['TASK'])\n",
    "    cleaned_output = text_output.replace(\"assistant\\n\", \"\")\n",
    "    print ('Inference output : ', cleaned_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173ae0de-06dd-4845-a8c9-8a625d7e6501",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_text = tokenizer.decode(200)\n",
    "token_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5bd3efd-4892-49cd-912e-3396a5472f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "print (input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2877b3cf-ae35-41f7-a9b5-59caf358ff09",
   "metadata": {},
   "outputs": [],
   "source": [
    "print (attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e082d8f-7fbc-4b0a-9e03-131b228975fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "eos_indices = (input_ids == tokenizer.eos_token_id).nonzero(as_tuple=True)[0]\n",
    "len(eos_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06faf16d-44e4-4d51-94d0-09b21e23e8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "eos_indices[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebdcae17-8f6f-4e96-b678-e11ba0c501c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_eos_index = eos_indices[0]\n",
    "labels[first_eos_index] = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6329a06-f5b2-4c8e-b04e-7a9c83811dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc47746-d0cf-4705-b754-3460d49b4ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_mask[first_eos_index] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "757315fe-e196-41de-ba4d-79e7a667a304",
   "metadata": {},
   "outputs": [],
   "source": [
    "print (attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95103d45-e7ab-4e7c-9b7a-c70036608c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019ef3b4-3fe2-4f64-a7f5-7ba7095e5aaf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
